# Nonce3Vec
This is an ongoing project on using informativeness binary rating to enhance
the performance of a fast-mapping machine, building on the existing
[nonce2vec](https://github.com/minimalparts/nonce2vec) by
[Herbelot & Baroni (2017)](http://aurelieherbelot.net/resources/papers/emnlp2017_final.pdf):
hence the name of questionable wit.

The code has been tested and Works On My Machine&#8482; with Python 3.6.5,
except for the open issues (if any) that are tracked on this repo. The project
is currently in development.

## 0. Settings
All settings, variables and filepaths you might want to change are stored in
`conf.py` as object properties, bearing intuitive names.

## 1. Setup the corpus files
For the purpose of training a classifier (a *Support Vector Machine*; below:
SVM), two corpora are used in this project: a small Wikipedia dump (10 000
first-sentences of so many pages), and a section of the
*British National Corpus* (below: BNC). The idea is to use these corpora as
examples of *informative* sentences and *generic* ones (average
informativeness), respectively. The choice of these corpora is provisional and
intended for testing purposes.

The Wikipedia dump can be found in `corpora/wiki/wikidump.txt`. For copyright
reasons, the BNC files are not provided. The programme is expecting to find it
in 10 compressed slices, named A.tar.gz to K.tar.gz, in `corpora/bnc/`.

The working corpora are generated from these sources by running:
```sh
$ python3 format_bnc.py
$ python3 format_wiki.py
```

`format_bnc.py` takes 5000 lines at random from the BNC, associates one random
word to each (to act as nonce), and writes the resulting corpus to
`corpora/bnc.txt`.

Similarly, `format_wiki.txt` picks 5000 lines at random from the Wikipedia dump,
prunes it of those lines that are empty, or just consist of disambiguation pages
(e.g. *"ENTRY may refer to"*), and then outputs to `corpora/wiki.txt`. The only
difference is that this corpus already has the lines paired to the relevant
nonces (the encyclopedic entries,i.e. the page titles); therefore, no word is
randomly picked to act as nonce.

Both script do a minimal sanitising (removing some anomalous characters and
multiple spaces) and discard lines with less than 3 words and punctuation signs
(to avoid keeping lines such as *word ;*).

For quick tests, even smaller files can be generated by running:
```sh
$ python3 mini_corpora.py
```

This generates the files `wiki_mini.txt` and `bnc_mini.txt` within `corpora/`.
These consist of the first 100 lines of the previously-generated working
corpora.

The size of the BNC subset (5000 lines) and of the `_mini` files (100) can be
changed from `conf.py`.

## 2. Create sentence vectors
Run:
```sh
$ python3 vectorize.py
```

This script takes the two appropriately formatted corpora, and
represents each sentence as an n-gram vector. It creates vectors for both word-
and POS-n-grams. It will dump a lot of logfiles (in `log/`), and two pickled
dictionaries (in `vec/`) for later use. Each dictionary contains the vector
representation of each sentence as the value of the coupled nonce (key).

`output/vec/` contains the output at various configurations for testing
purposes. See the `contents.md` contained in the folder for further details.

## 3. Run SVM to classify vectors
Run:
```sh
$ python3 -u classify.py > svm/stdout.txt
```

This loads the pickled files, prints the number of available vectors for each
condition (generic vs. informative), and asks the user to choose the amplitude
of the two training sets. Then, it runs a SVM thereon to learn to tag sentences
between *informative* and *generic*. It then outputs the results and dumps
plotted images in `svm/`.

The `-u` flag disables buffering: since the output is generated in different
ways (half by `sklearn.svm.SVC()` and half by `print()`), buffering the output
would result in tangled up output.

`output/svm/` contains the output at various configurations for
reference purposes. See the `contents.md` contained in the folder for further
details.